{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkrueger/Redis-Workshops/blob/main/04-Large_Language_Model/04-Large_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AkZOYLVbc6m"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "![Redis](https://redis.com/wp-content/themes/wpx/assets/images/logo-redis.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "In this notebook you'll be using two LLMs. OpenAI ChatGPT `gpt-3.5-turbo` and self-hosted, in-notebook, `databricks/dolly-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBmXNV2Qpkhe"
      },
      "outputs": [],
      "source": [
        "%pip -q install openai accelerate transformers sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUa-ZQkNchnc"
      },
      "source": [
        "Initialize OpenAI. You need to supply your OpenAI API key (starts with `sk-...`) when prompted. You can find your API key at https://platform.openai.com/account/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8R4COiD08Ux"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
        "if OPENAI_API_KEY == \"\":\n",
        "    key=getpass.getpass(prompt='OpenAI Key: ', stream=None)\n",
        "    os.environ['OPENAI_API_KEY']=key\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-lVNefzcxRq"
      },
      "source": [
        "Initialize `databricks/dolly-v2-3b` via [HuggingFace](https://huggingface.co/databricks/dolly-v2-3b). Multiple progressively more powerful models are available, including 3b, 7b and 12b (referring to Billions of parameters). `dolly-v2-3b` is the only model in the family that would fit in the memory and GPU available in a free Google Colab instance.\n",
        "\n",
        "Loading and initializing the model can take few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBZG4fmnpoiR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "dolly_completion = pipeline(model=\"databricks/dolly-v2-3b\",\n",
        "                         torch_dtype=torch.bfloat16,\n",
        "                         trust_remote_code=True,\n",
        "                         device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kBXv65d2Sj"
      },
      "source": [
        "Helper function for OpenAI ChatGPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73GvlwaM07b-"
      },
      "outputs": [],
      "source": [
        "def openai_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAY1sbW-d3os"
      },
      "source": [
        "# Create the prompt\n",
        "\n",
        "Prompt contains instructions, context and the question. Feel free to experiment with the prompt and see the difference in responses from different models.\n",
        "\n",
        "News article used in this example: https://www.cnn.com/2023/05/18/media/disney-florida-desantis/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwO9x-kypsst"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "\n",
        "Disney on Thursday upped the ante in its battle with Florida’s Republican Gov. Ron DeSantis, and it cost his state 2,000 white-collar jobs.\n",
        "Disney is scrapping plans to build a $1 billion office complex in Florida, citing “changing business conditions,” according to a memo provided by a Disney spokesperson.\n",
        "The decision comes at a time when the company is openly feuding with DeSantis, who is expected to officially enter the 2024 GOP presidential race next week, CNN reported Thursday.\n",
        "A spokesperson for DeSantis said it was “unsurprising” that Disney would cancel the project “given the company’s financial straits, falling market cap and declining stock price.”\n",
        "Disney, along with the broader media industry, is grappling with a difficult advertising environment and a massive writers strike. Earlier this year it announced it would be cutting 7,000 jobs as part of a cost-cutting effort.\n",
        "Separately, the company confirmed Thursday that it would shut down its Star Wars: Galactic Starcruiser resort at Disney World just over a year after it opened.\n",
        "The popular attraction “will take its final voyage” at the end of September, Disney said, adding that it is working with guests to rebook reservations for later in the year.\n",
        "\"\"\"\n",
        "\n",
        "question=\"What plans Disney is cancelling?\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Instruction: Use only information in the following context to answer the question at the end.\n",
        "If you don't know, say that you do not know.\n",
        "\n",
        "Context:  {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response:\n",
        "\"\"\"\n",
        "print(prompt)\n",
        "\n",
        "res = dolly_completion(prompt)\n",
        "print(\"Dolly:\")\n",
        "print(res[0]['generated_text'])\n",
        "\n",
        "\n",
        "res = openai_completion(prompt)\n",
        "print(\"\\nOpenAI:\")\n",
        "print(res.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBEIzCOCZUBB"
      },
      "source": [
        "## TODO:\n",
        "\n",
        "Some ideas for you to try:\n",
        "- add \"Respond in French/Spanish\" to the prompt.\n",
        "\n",
        "-  more information into the context until you hit the token limit of the model.\n",
        "\n",
        "- Replace the entire prompt with a simple task like \"Tell me about Newmarket, Ontario\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWnI5NVkfa8L"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me about Newmarket, Ontario\"\n",
        "\n",
        "res = dolly_completion(prompt)\n",
        "print(\"Dolly:\")\n",
        "print(res[0]['generated_text'])\n",
        "\n",
        "\n",
        "res = openai_completion(prompt)\n",
        "print(\"\\nOpenAI:\")\n",
        "print(res.choices[0].message[\"content\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}