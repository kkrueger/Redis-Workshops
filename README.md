# Semantic Caching using Redis VL
![Redis](https://redis.com/wp-content/themes/wpx/assets/images/logo-redis.svg?auto=webp&quality=85,75&width=120)

This notebook uses RedisVL to turn Redis, with its vector search capability, into a semantic cache to store query results, thereby reducing the number of requests and tokens sent to the Large Language Models (LLM) service. This decreases expenses and enhances performance by reducing the time taken to generate responses.

This notebook requires an OpenAI API key. You can find your API key at https://platform.openai.com/account/api-keys.
