{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkrueger/Redis-Workshops/blob/main/07-Semantic_Caching_Redis/07-Semantic_Caching_Redis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AkZOYLVbc6m"
      },
      "source": [
        "# Semantic Caching with Redis\n",
        "\n",
        "![Redis](https://redis.com/wp-content/themes/wpx/assets/images/logo-redis.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "RedisVL provides the `LLMCache` interface to turn Redis, with its vector search capability, into a semantic cache to store query results, thereby reducing the number of requests and tokens sent to the Large Language Models (LLM) service. This decreases expenses and enhances performance by reducing the time taken to generate responses.\n",
        "\n",
        "This notebook will go over how to use `LLMCache` for your applications\n",
        "\n",
        "First, we will install Python dependencies and import OpenAI to user their API for responding to prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBmXNV2Qpkhe"
      },
      "outputs": [],
      "source": [
        "%pip -q install openai redisvl transformers sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUa-ZQkNchnc"
      },
      "source": [
        "Initialize OpenAI. You need to supply your OpenAI API key (starts with `sk-...`) when prompted. You can find your API key at https://platform.openai.com/account/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8R4COiD08Ux"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import getpass\n",
        "import redis\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
        "if OPENAI_API_KEY == \"\":\n",
        "    key=getpass.getpass(prompt='OpenAI Key: ', stream=None)\n",
        "    os.environ['OPENAI_API_KEY']=key\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fkzh1COlMSK"
      },
      "source": [
        "### Install Redis Stack\n",
        "\n",
        "Redis Search will be used as Vector Similarity Search engine for LangChain. Instead of using in-notebook Redis Stack https://redis.io/docs/getting-started/install-stack/ you can provision your own free instance of Redis in the cloud. Get your own Free Redis Cloud instance at https://redis.com/try-free/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fowy4iKxgrTR"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hQI41L9lMSL"
      },
      "source": [
        "### Connect to Redis\n",
        "\n",
        "By default this notebook would connect to the local instance of Redis Stack. If you have your own Redis Cloud instance - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww9kR1QegsQV"
      },
      "outputs": [],
      "source": [
        "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
        "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")\n",
        "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")\n",
        "#Replace values above with your own if using Redis Cloud instance\n",
        "#REDIS_HOST=\"\"\n",
        "#REDIS_PORT=\n",
        "#REDIS_PASSWORD=\"\"\n",
        "\n",
        "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBZG4fmnpoiR"
      },
      "outputs": [],
      "source": [
        "# Helper method for submitting a prompt to OpenAI\n",
        "def ask_openai(question):\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"text-davinci-003\",\n",
        "      prompt=question,\n",
        "      max_tokens=200\n",
        "    )\n",
        "    return response.choices[0].text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73GvlwaM07b-"
      },
      "outputs": [],
      "source": [
        "# Test it\n",
        "print(ask_openai(\"What is the capital of France?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAY1sbW-d3os"
      },
      "source": [
        "# Initializing and using `LLMCache`\n",
        "\n",
        "`LLMCache` will automatically create an index within Redis upon initialization for the semantic cache. The same `SearchIndex` class used in the previous tutorials is used here to perform index creation and manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwO9x-kypsst"
      },
      "outputs": [],
      "source": [
        "from redisvl.llmcache.semantic import SemanticCache\n",
        "cache = SemanticCache(\n",
        "    redis_url=REDIS_URL,\n",
        "    threshold=0.9, # semantic similarity threshold\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWnI5NVkfa8L"
      },
      "outputs": [],
      "source": [
        "# Look at the index specification created for the semantic cache lookup\n",
        "!rvl index info -i cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the cache"
      ],
      "metadata": {
        "id": "MTepZauIBKsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the cache\n",
        "cache.check(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "DW0Wk_ymBOIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the question and answer\n",
        "cache.store(\"What is the capital of France?\", \"Paris\")"
      ],
      "metadata": {
        "id": "wdKORp5eB1KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the cache again\n",
        "cache.check(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "bR6RHCXwB69V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for a semantically similar result\n",
        "cache.check(\"What really is the capital of France?\")"
      ],
      "metadata": {
        "id": "if8ABmAOCAKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrease the semantic similarity threshold\n",
        "cache.set_threshold(0.7)\n",
        "cache.check(\"What really is the capital of France?\")"
      ],
      "metadata": {
        "id": "w2DyYQVeCQaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial example (not semantically similar enough)\n",
        "cache.check(\"What is the capital of Spain?\")"
      ],
      "metadata": {
        "id": "IaOrjrDDC-Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache.clear()"
      ],
      "metadata": {
        "id": "pWmTy0JPDFIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo1f4keDEUiP"
      },
      "source": [
        "## Performance\n",
        "\n",
        "Next, we will measure the speedup obtained by using ``LLMCache``. We will use the ``time`` module to measure the time taken to generate responses with and without ``LLMCache``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBb5nV31EUiP"
      },
      "outputs": [],
      "source": [
        "def answer_question(question: str):\n",
        "    results = cache.check(question)\n",
        "    if results:\n",
        "        return results[0]\n",
        "    else:\n",
        "        answer = ask_openai(question)\n",
        "        cache.store(question, answer)\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KohO3KUXEUiP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "answer = answer_question(\"What is the capital of France?\")\n",
        "end = time.time()\n",
        "print(f\"Time taken without cache {time.time() - start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD4RwbZ7EUiP"
      },
      "outputs": [],
      "source": [
        "cached_start = time.time()\n",
        "cached_answer = answer_question(\"What is the capital of France?\")\n",
        "cached_end = time.time()\n",
        "print(f\"Time Taken with cache: {cached_end - cached_start}\")\n",
        "print(f\"Percentage of time saved: {round(((end - start) - (cached_end - cached_start)) / (end - start) * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ4WuvPCEUiQ"
      },
      "outputs": [],
      "source": [
        "# check the stats of the index\n",
        "!rvl stats -i cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mluIFc1fEUiQ"
      },
      "outputs": [],
      "source": [
        "# remove the index and all cached items\n",
        "cache.index.delete()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}